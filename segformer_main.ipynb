{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viratcode/Udayachal_Project_code/blob/Model_Training_Path/segformer_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK4r6cPOQeFz"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkrZ_0NvQ2GL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda import amp\n",
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNm6bR8Wc3Vp"
      },
      "outputs": [],
      "source": [
        "from albumentations import Compose, HorizontalFlip, VerticalFlip\n",
        "\n",
        "transform = Compose([\n",
        "    HorizontalFlip(p=0.5),  # For horizontal flips\n",
        "    VerticalFlip(p=0.5)   # For vertical flips (uncomment if needed)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV3JhHOjQ6Ol"
      },
      "outputs": [],
      "source": [
        "WIDTH = 256\n",
        "HEIGHT = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBWgY0BEYwYp"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import Dataset\n",
        "# import os\n",
        "# import cv2\n",
        "# import albumentations as aug\n",
        "\n",
        "# class ImageSegmentationDataset(Dataset):\n",
        "#     \"\"\"Image segmentation dataset.\"\"\"\n",
        "\n",
        "#     def __init__(self, root_dir, feature_extractor, transforms=None, split=\"train\"):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             root_dir (string): Root directory of the dataset.\n",
        "#             feature_extractor (SegFormerFeatureExtractor): Feature extractor.\n",
        "#             transforms (albumentations.Compose): Data augmentations.\n",
        "#             split (string): \"train\", \"val\", or \"test\" to indicate the split.\n",
        "#         \"\"\"\n",
        "#         self.root_dir = root_dir\n",
        "#         self.feature_extractor = feature_extractor\n",
        "#         self.transforms = transforms\n",
        "#         self.split = split\n",
        "\n",
        "#         # Assuming images in 'images/train' and masks in 'mask/train'\n",
        "#         self.img_dir = os.path.join(self.root_dir, \"source\")\n",
        "#         self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
        "\n",
        "#         # Read image and annotation file names\n",
        "#         self.images = sorted(os.listdir(self.img_dir))\n",
        "#         self.annotations = sorted(os.listdir(self.ann_dir))\n",
        "\n",
        "#         assert len(self.images) == len(self.annotations), \"Unequal number of images and masks\"\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.images)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "\n",
        "#         image_path = os.path.join(self.img_dir, self.images[idx])\n",
        "#         mask_path = os.path.join(self.ann_dir, self.annotations[idx])\n",
        "\n",
        "#         image = cv2.imread(image_path)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         segmentation_map = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "#         image = cv2.resize(image, (WIDTH, HEIGHT), interpolation=cv2.INTER_LINEAR)\n",
        "#         segmentation_map = cv2.resize(segmentation_map, (WIDTH, HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "#         # Apply transforms based on split\n",
        "#         if self.split == \"train\" and self.transforms is not None:\n",
        "#             augmented = self.transforms(image=image, mask=segmentation_map)\n",
        "#             image, segmentation_map = augmented['image'], augmented['mask']\n",
        "\n",
        "#         encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "#         # Remove batch dimension\n",
        "#         for k, v in encoded_inputs.items():\n",
        "#             encoded_inputs[k].squeeze_()\n",
        "\n",
        "#         return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwRjHA9Ivmfr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import cv2\n",
        "import albumentations as aug\n",
        "\n",
        "class ImageSegmentationDataset(Dataset):\n",
        "    \"\"\"Image segmentation dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, feature_extractor, transforms=None, split=\"train\", image_size=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory of the dataset.\n",
        "            feature_extractor (SegFormerFeatureExtractor): Feature extractor.\n",
        "            transforms (albumentations.Compose): Data augmentations.\n",
        "            split (string): \"train\", \"val\", or \"test\" to indicate the split.\n",
        "            image_size (int): Desired size for resizing images (default: 256).\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.transforms = transforms\n",
        "        self.split = split\n",
        "        self.image_size = image_size  # Store image size\n",
        "\n",
        "        # Assuming images in 'images/train' and masks in 'mask/train'\n",
        "        self.img_dir = os.path.join(self.root_dir, \"source\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
        "\n",
        "        # Read image and annotation file names\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "        self.annotations = sorted(os.listdir(self.ann_dir))\n",
        "\n",
        "        assert len(self.images) == len(self.annotations), \"Unequal number of images and masks\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path = os.path.join(self.img_dir, self.images[idx])\n",
        "        mask_path = os.path.join(self.ann_dir, self.annotations[idx])\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        segmentation_map = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Resize images to the specified image_size\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n",
        "        segmentation_map = cv2.resize(segmentation_map, (self.image_size, self.image_size), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Apply transforms based on split\n",
        "        if self.split == \"train\" and self.transforms is not None:\n",
        "            augmented = self.transforms(image=image, mask=segmentation_map)\n",
        "            image, segmentation_map = augmented['image'], augmented['mask']\n",
        "\n",
        "        encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "        # Remove batch dimension\n",
        "        for k, v in encoded_inputs.items():\n",
        "            encoded_inputs[k].squeeze_()\n",
        "\n",
        "        return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZCRSYBoeKI1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anvqQuoweLa1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from albumentations import Compose, HorizontalFlip, VerticalFlip\n",
        "from transformers import SegformerFeatureExtractor\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Data augmentation transforms\n",
        "transform = Compose([\n",
        "    HorizontalFlip(p=0.5),\n",
        "    VerticalFlip(p=0.5)\n",
        "])\n",
        "\n",
        "# Define root directory and feature extractor\n",
        "root_dir = 'your dataset'\n",
        "feature_extractor = SegformerFeatureExtractor(size=256, align=False, reduce_zero_label=False)\n",
        "\n",
        "# Create the full dataset\n",
        "dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)\n",
        "\n",
        "# Split the dataset with a fixed seed\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.7 * dataset_size)\n",
        "val_size = int(0.15 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "generator = torch.Generator().manual_seed(seed)\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "valid_dataloader = DataLoader(val_dataset, batch_size=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXeygCbeeLWI"
      },
      "outputs": [],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgXcLO9ui_CA"
      },
      "outputs": [],
      "source": [
        "encoded_inputs = train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7-PRgs2jDI-"
      },
      "outputs": [],
      "source": [
        "encoded_inputs[\"pixel_values\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cue04DYFjHlD"
      },
      "outputs": [],
      "source": [
        "encoded_inputs[\"labels\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUxKulTIjPaB"
      },
      "outputs": [],
      "source": [
        "encoded_inputs[\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmbtdIckl9vR"
      },
      "outputs": [],
      "source": [
        "encoded_inputs[\"labels\"].squeeze().unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnA9_nlgl-T6"
      },
      "outputs": [],
      "source": [
        "mask = encoded_inputs[\"labels\"].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH5fiqiYnEg0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymDmzQEgnHZI"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loADIsAynMQ8"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93rPTQXDnZ4U"
      },
      "outputs": [],
      "source": [
        "for k,v in batch.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR4NAuPCnerr"
      },
      "outputs": [],
      "source": [
        "batch[\"labels\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McY1uG9IydDB"
      },
      "outputs": [],
      "source": [
        "# Install or upgrade PyTorch to a version >= 2.6\n",
        "!pip install --upgrade torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG-g6-UwUO6H"
      },
      "outputs": [],
      "source": [
        "# Assuming your dataset has 2 classes (0 for background, 1 for building)\n",
        "id2label = {0: \"background\", 1: \"building\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "# Update model initialization with a larger SegFormer model (e.g., mit-b5)\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    \"nvidia/mit-b5\",  # Changed from mit-b0 to mit-b5 (or another larger version)\n",
        "    ignore_mismatched_sizes=True,\n",
        "    num_labels=len(id2label),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    reshape_last_stage=True,\n",
        "    image_size=256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BLatYhkkpJYL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from torch.optim import AdamW # Import AdamW from transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c3Rygfbo5Gb"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=0.00006)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Model Initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwU4W3uSU2ZE"
      },
      "outputs": [],
      "source": [
        "!pip install torch>=2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYH4UobA4w00"
      },
      "source": [
        "#Training Model- no Need To execute Everytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "7cHRk0iR0J_5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  # Example using CrossEntropyLoss\n",
        "accumulation_steps = 8\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "for epoch in range(1, 11):  # loop over the dataset multiple times\n",
        "    print(\"Epoch:\", epoch)\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(pbar):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        print(\"Before modification:\")\n",
        "        print(\"Minimum label value:\", labels.min().item())\n",
        "        print(\"Maximum label value:\", labels.max().item())\n",
        "        # Ensure labels only contain 0 and 1 (binary segmentation)\n",
        "        labels = labels.long()  # Cast labels to Long type\n",
        "        labels[labels > 1] = 0  # Force any values > 1 to be 0 (background)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ---start of changes---\n",
        "        # forward pass\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # interpolate the logits to the same size as the labels\n",
        "        upsampled_logits = nn.functional.interpolate(\n",
        "            outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        # ---end of changes---\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # calculate loss with upsampled logits\n",
        "            loss = criterion(upsampled_logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # evaluate (use upsampled logits here as well)\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "        # Calculate loss using the weighted criterion\n",
        "        loss = criterion(upsampled_logits, labels)  # Use upsampled logits here as well\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
        "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "        true_labels = labels[mask].detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(pred_labels, true_labels)\n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss.item())\n",
        "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
        "\n",
        "        # backward + optimize\n",
        "        if (idx + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()                            # Now we can do an optimizer step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    else:\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for idx, batch in enumerate(valid_dataloader):\n",
        "          pixel_values = batch[\"pixel_values\"].to(device)\n",
        "          labels = batch[\"labels\"].to(device)\n",
        "\n",
        "          # Ensure labels only contain 0 and 1 for validation as well\n",
        "          labels = labels.long()\n",
        "          labels[labels > 1] = 0\n",
        "\n",
        "          outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "          upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "          predicted = upsampled_logits.argmax(dim=1)\n",
        "          mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
        "          pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "          true_labels = labels[mask].detach().cpu().numpy()\n",
        "          accuracy = accuracy_score(pred_labels, true_labels)\n",
        "          val_loss = outputs.loss\n",
        "          val_accuracies.append(accuracy)\n",
        "          val_losses.append(val_loss.item())\n",
        "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
        "         Train Loss: {sum(losses)/len(losses)}\\\n",
        "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
        "         Val Loss: {sum(val_losses)/len(val_losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoA0iT64seOh"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Model Initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZd-SeXuACns"
      },
      "source": [
        "#Training part No need to execute every time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R9AUg-rrruxZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.optim as optim\n",
        "\n",
        "# Use CPU as the device\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Initialize GradScaler, but without using AMP since we're on CPU\n",
        "scaler = GradScaler(enabled=False)  # No AMP on CPU\n",
        "\n",
        "# Define the optimizer (SGD in your case)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 21):  # loop over the dataset multiple times\n",
        "    print(\"Epoch:\", epoch)\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "    val_ious = []  # To store IoU values for validation\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(pbar):\n",
        "        # get the inputs\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Ensure labels only contain 0 and 1 (binary segmentation)\n",
        "        labels = labels.long()  # Cast labels to Long type\n",
        "        labels[labels > 1] = 0  # Force any values > 1 to be 0 (background)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # interpolate the logits to the same size as the labels\n",
        "        upsampled_logits = nn.functional.interpolate(\n",
        "            outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "\n",
        "        # No AMP: Direct loss calculation\n",
        "        loss = criterion(upsampled_logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # evaluate (use upsampled logits here as well)\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "        # Calculate loss using the weighted criterion\n",
        "        loss = criterion(upsampled_logits, labels)  # Use upsampled logits here as well\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        mask = (labels != 255)  # we don't include the background class in the accuracy calculation\n",
        "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "        true_labels = labels[mask].detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(pred_labels, true_labels)\n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Calculate IoU for this batch\n",
        "        iou_value = compute_iou(predicted, labels)\n",
        "        val_ious.append(iou_value.item())  # Store IoU for this batch\n",
        "\n",
        "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
        "\n",
        "    # Validation phase\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for idx, batch in enumerate(valid_dataloader):\n",
        "                pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                # Ensure labels only contain 0 and 1 for validation as well\n",
        "                labels = labels.long()\n",
        "                labels[labels > 1] = 0\n",
        "\n",
        "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "                predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "                mask = (labels != 255)  # we don't include the background class in the accuracy calculation\n",
        "                pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "                true_labels = labels[mask].detach().cpu().numpy()\n",
        "                accuracy = accuracy_score(pred_labels, true_labels)\n",
        "                val_loss = outputs.loss\n",
        "                val_accuracies.append(accuracy)\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "                # Calculate IoU for the validation batch\n",
        "                iou_value = compute_iou(predicted, labels)\n",
        "                val_ious.append(iou_value.item())\n",
        "\n",
        "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
        "         Train Loss: {sum(losses)/len(losses)}\\\n",
        "         Train IoU: {sum(val_ious)/len(val_ious)}\\\n",
        "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
        "         Val Loss: {sum(val_losses)/len(val_losses)}\\\n",
        "         Val IoU: {sum(val_ious)/len(val_ious)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9qJadgssMLA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def compute_iou(pred, target, num_classes=2):\n",
        "    \"\"\"\n",
        "    Compute the Intersection over Union (IoU) for a binary or multi-class segmentation task.\n",
        "\n",
        "    Args:\n",
        "        pred (Tensor): The predicted tensor of shape [batch_size, height, width]\n",
        "        target (Tensor): The ground truth tensor of shape [batch_size, height, width]\n",
        "        num_classes (int): Number of classes in the segmentation task (default is 2 for binary)\n",
        "\n",
        "    Returns:\n",
        "        iou (Tensor): The IoU for each class, of shape [num_classes]\n",
        "    \"\"\"\n",
        "    iou = torch.zeros(num_classes).to(pred.device)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        # Create binary masks for each class (foreground = 1, background = 0)\n",
        "        pred_class = (pred == cls).float()\n",
        "        target_class = (target == cls).float()\n",
        "\n",
        "        # Compute intersection and union for this class\n",
        "        intersection = (pred_class * target_class).sum()\n",
        "        union = pred_class.sum() + target_class.sum() - intersection\n",
        "\n",
        "        # Avoid division by zero by ensuring the union is not zero\n",
        "        iou[cls] = intersection / (union + 1e-6)  # Adding epsilon to prevent divide-by-zero errors\n",
        "\n",
        "    return iou.mean()  # Return the mean IoU over all classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OX4G0dTSyVC"
      },
      "outputs": [],
      "source": [
        "model_path = \"where model is to be saved\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "print(f\"Model loaded from {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwKkc4ZzT9UD"
      },
      "outputs": [],
      "source": [
        "#show results of this saved model, take any one random imgae  , show accuracy and loss of predicted output too\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have the necessary variables and functions defined from the previous code\n",
        "# ... (including model, feature_extractor, device, etc.)\n",
        "\n",
        "# Choose a random image from the test dataset\n",
        "random_image_index = random.randint(0, len(test_dataset) - 1)\n",
        "encoded_inputs = test_dataset[random_image_index]\n",
        "\n",
        "# Move inputs to the device\n",
        "pixel_values = encoded_inputs[\"pixel_values\"].unsqueeze(0).to(device)  # Add batch dimension\n",
        "labels = encoded_inputs[\"labels\"].unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "# Ensure labels only contain 0 and 1 for validation as well\n",
        "labels = labels.long()\n",
        "labels[labels > 1] = 0\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "    upsampled_logits = nn.functional.interpolate(\n",
        "        outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "    predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "# Calculate accuracy and loss\n",
        "mask = (labels != 255)\n",
        "pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "true_labels = labels[mask].detach().cpu().numpy()\n",
        "accuracy = accuracy_score(pred_labels, true_labels)\n",
        "loss = outputs.loss.item()\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Loss: {loss}\")\n",
        "\n",
        "# Display the original image, ground truth mask, and predicted mask\n",
        "original_image = Image.fromarray(np.transpose(pixel_values.squeeze(0).cpu().numpy(), (1, 2, 0)).astype(np.uint8))\n",
        "ground_truth_mask = Image.fromarray(labels.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "predicted_mask = Image.fromarray(predicted.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(original_image)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Ground Truth Mask\")\n",
        "plt.imshow(ground_truth_mask, cmap=\"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.imshow(predicted_mask, cmap=\"gray\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3k8Ms2Ri2B"
      },
      "source": [
        "# Try to Coorect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ7H1SlWQ9sY"
      },
      "outputs": [],
      "source": [
        "# Choose a random image from the test dataset\n",
        "random_image_index = random.randint(0, len(test_dataset) - 1)\n",
        "encoded_inputs = test_dataset[random_image_index]\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Predicted Mask\")\n",
        "\n",
        "plt.imshow(encoded_inputs['pixel_values'].squeeze(0).cpu().numpy().transpose(1, 2, 0).astype('uint8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0yY3B2mIL_rT"
      },
      "outputs": [],
      "source": [
        "#test the stored model on test data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Create an empty list to store results\n",
        "results = []\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Ensure labels only contain 0 and 1 for testing as well\n",
        "        labels = labels.long()\n",
        "        labels[labels > 1] = 0\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        upsampled_logits = nn.functional.interpolate(\n",
        "            outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        mask = (labels != 255)\n",
        "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "        true_labels = labels[mask].detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(pred_labels, true_labels)\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = jaccard_score(true_labels, pred_labels, average='macro') # or 'weighted'\n",
        "\n",
        "        loss = outputs.loss.item()\n",
        "\n",
        "        results.append({\n",
        "            'Image Index': idx,\n",
        "            'Accuracy': accuracy,\n",
        "            'IoU': iou,\n",
        "            'Loss': loss\n",
        "        })\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results to an Excel file\n",
        "results_df.to_excel('test_results.xlsx', index=False)\n",
        "print(\"Test results saved to test_results.xlsx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y3WmFdvidnaf"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store results\n",
        "results = []\n",
        "\n",
        "# Test loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Ensure labels only contain 0 and 1 for testing as well\n",
        "        labels = labels.long()\n",
        "        labels[labels > 1] = 0\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        upsampled_logits = nn.functional.interpolate(\n",
        "            outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        mask = (labels != 255)\n",
        "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "        true_labels = labels[mask].detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(pred_labels, true_labels)\n",
        "\n",
        "        # Calculate IoU\n",
        "        iou = jaccard_score(true_labels, pred_labels, average='macro') # or 'weighted'\n",
        "\n",
        "        loss = outputs.loss.item()\n",
        "\n",
        "        results.append({\n",
        "            'Image Index': idx,\n",
        "            'Accuracy': accuracy,\n",
        "            'IoU': iou,\n",
        "            'Loss': loss\n",
        "        })\n",
        "\n",
        "# Create a pandas DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the results DataFrame\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4etkui8zcUGa"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['Loss'].plot(kind='hist', bins=20, title='Loss')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbCQVTPycSis"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['IoU'].plot(kind='hist', bins=20, title='IoU')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ7kyKn1xGnM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qEIvPzvcRHp"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['Accuracy'].plot(kind='hist', bins=20, title='Accuracy')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWnLMkObcOTO"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['Image Index'].plot(kind='hist', bins=20, title='Image Index')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmELz_gdcL3A"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df.plot(kind='scatter', x='IoU', y='Loss', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za4g-B2gcIZL"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df.plot(kind='scatter', x='Accuracy', y='IoU', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3y7nsJkcE6k"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df.plot(kind='scatter', x='Image Index', y='Accuracy', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpSGSOZKb8lA"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['Loss'].plot(kind='line', figsize=(8, 4), title='Loss')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbIK6yHgb6Xo"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['IoU'].plot(kind='line', figsize=(8, 4), title='IoU')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q60EmpOyb2Tm"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['Accuracy'].plot(kind='line', figsize=(8, 4), title='Accuracy')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QJEGd0abz1J"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "results_df['Image Index'].plot(kind='line', figsize=(8, 4), title='Image Index')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSg-LVZ9AfE5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Choose a random image from the test dataset\n",
        "random_image_index = random.randint(0, len(test_dataset) - 1)\n",
        "encoded_inputs = test_dataset[random_image_index]\n",
        "\n",
        "# Move inputs to the device\n",
        "pixel_values = encoded_inputs[\"pixel_values\"].unsqueeze(0).to(device)  # Add batch dimension\n",
        "labels = encoded_inputs[\"labels\"].unsqueeze(0).to(device)\n",
        "\n",
        "# Ensure labels only contain 0 and 1 for testing as well\n",
        "labels = labels.long()\n",
        "labels[labels > 1] = 0\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "    upsampled_logits = nn.functional.interpolate(\n",
        "        outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "    predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "# ... (rest of your existing code for calculations and saving results) ...\n",
        "\n",
        "\n",
        "# Display the original image, ground truth mask, and predicted mask\n",
        "original_image = Image.fromarray(np.transpose(pixel_values.squeeze(0).cpu().numpy(), (1, 2, 0)).astype(np.uint8))\n",
        "ground_truth_mask = Image.fromarray(labels.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "predicted_mask = Image.fromarray(predicted.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(original_image)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Ground Truth Mask\")\n",
        "plt.imshow(ground_truth_mask, cmap=\"gray\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.imshow(predicted_mask, cmap=\"gray\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6ilpRKbKhuO"
      },
      "outputs": [],
      "source": [
        "#result visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# ... (your existing code) ...\n",
        "\n",
        "# Choose a random image from the test dataset\n",
        "random_image_index = random.randint(0, len(test_dataset) - 1)\n",
        "encoded_inputs = test_dataset[random_image_index]\n",
        "\n",
        "# Move inputs to the device\n",
        "pixel_values = encoded_inputs[\"pixel_values\"].unsqueeze(0).to(device)  # Add batch dimension\n",
        "labels = encoded_inputs[\"labels\"].unsqueeze(0).to(device)\n",
        "\n",
        "# Ensure labels only contain 0 and 1 for testing as well\n",
        "# ***This is where the distortion might happen if your original masks have more than 2 classes***\n",
        "# labels = labels.long()\n",
        "# labels[labels > 1] = 0\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "    upsampled_logits = nn.functional.interpolate(\n",
        "        outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "    predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "# ... (rest of your existing code for calculations and saving results) ...\n",
        "\n",
        "\n",
        "# Display the original image, ground truth mask, and predicted mask\n",
        "original_image = Image.fromarray(np.transpose(pixel_values.squeeze(0).cpu().numpy(), (1, 2, 0)).astype(np.uint8))\n",
        "# ***Modified to keep original mask values***\n",
        "ground_truth_mask = Image.fromarray(encoded_inputs[\"labels\"].cpu().numpy().astype(np.uint8))\n",
        "predicted_mask = Image.fromarray(predicted.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(original_image)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Ground Truth Mask\")\n",
        "plt.imshow(ground_truth_mask, cmap=\"gray\") # You can adjust the colormap if needed\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.imshow(predicted_mask, cmap=\"gray\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}